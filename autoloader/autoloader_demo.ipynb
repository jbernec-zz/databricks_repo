{"cells":[{"cell_type":"markdown","source":["# The code in this notebook explores the Spark Autoloader feature and Structured Streaming API"],"metadata":{}},{"cell_type":"code","source":["# Configure SPN direct access to ADLS Gen 2. Permission is based on File or folder based ACL assignments to the Data Lake filesystem (container) .\n# RBAC assignments to the top level Azure Data Lake resource is not required.\nspark.conf.set(\"fs.azure.account.auth.type.dstore.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.dstore.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.dstore.dfs.core.windows.net\", dbutils.secrets.get(\"myscope\", key=\"clientid\"))\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.dstore.dfs.core.windows.net\", dbutils.secrets.get(\"myscope\", key=\"clientsecret\"))\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.dstore.dfs.core.windows.net\", \"https://login.microsoftonline.com/{}/oauth2/token\".format(dbutils.secrets.get(\"myscope\", key=\"tenantid\")))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Read a single csv file into dataframe to retrieve current schema\ndf_csv = spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"abfss://rawcontainer@dstore.dfs.core.windows.net/mockcsv/file1.csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Display schema\ndf_csv.schema"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Display the object type\ntype(df_csv.schema)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Save the schema to a json file\nimport json\nwith open(\"/dbfs/tmp/csvschema.json\", \"w\") as f:\n          json.dump(df_csv.schema.jsonValue(), f)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Test reading the schema from the json file\nfrom pyspark.sql.types import StructType\nwith open(\"/dbfs/tmp/csvschema.json\") as sch_file:\n    dataset_schema = StructType.fromJson(json.load(sch_file))\n    #print(dataset_schema.simpleString())\ndataset_schema"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Test the schema object type retrieved from the json file\ntype(dataset_schema)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Import required modules\nfrom pyspark.sql import functions as f\nfrom datetime import datetime\nfrom pyspark.sql.types import StringType"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["connection_string = dbutils.secrets.get(\"myscope\", key=\"storageconnstr\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df_stream_in = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.useNotifications\", True).option(\"cloudFiles.format\", \"csv\")\\\n            .option(\"cloudFiles.connectionString\", connection_string)\\\n            .option(\"cloudFiles.resourceGroup\", \"rganalytics\")\\\n            .option(\"cloudFiles.subscriptionId\", dbutils.secrets.get(\"myscope\", key=\"subid\"))\\\n            .option(\"cloudFiles.tenantId\", dbutils.secrets.get(\"myscope\", key=\"tenantid\"))\\\n            .option(\"cloudFiles.clientId\", dbutils.secrets.get(\"myscope\", key=\"clientid\"))\\\n            .option(\"cloudFiles.clientSecret\", dbutils.secrets.get(\"myscope\", key=\"clientsecret\"))\\\n            .option(\"cloudFiles.region\", \"eastus\")\\\n            .schema(dataset_schema)\\\n            .option(\"cloudFiles.includeExistingFiles\", True).load(\"abfss://rawcontainer@dstore.dfs.core.windows.net/mockcsv/\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Custom function\ndef time_col():\n  pass\n  return datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Creat a UDF Function from the custom function above\ntime_col_udf = spark.udf.register(\"time_col_sql_udf\", time_col, StringType())"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Add new constant column using the UDF above\ndf_transform = df_stream_in.withColumn(\"time_col\", f.lit(time_col_udf()))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Output Dataframe to JSON files\ndf_out = df_transform.writeStream\\\n  .trigger(once=True)\\\n  .format(\"json\")\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", \"abfss://checkpointcontainer@dstore.dfs.core.windows.net/checkpointcsv\")\\\n  .start(\"abfss://rawcontainer@dstore.dfs.core.windows.net/autoloaderjson\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Custom function to output streaming dataframe to Azure SQL DB\ndef output_sqldb(batch_df, batch_id):\n#   Set Azure DW Properties and Conn String\n  sql_pwd = dbutils.secrets.get(scope = \"myscope\", key = \"sqlpwd\")\n  sql_user = dbutils.secrets.get(scope = \"myscope\", key = \"sqluser\")\n  dbtable = \"staging\"\n  jdbc_url = \"jdbc:sqlserver://sqlserver09.database.windows.net:1433;database=demodb;user=\"+ sql_user +\";password=\"+ sql_pwd +\";encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n  # write the pyspark dataframe to Azure DW\n  batch_df.write.format(\"jdbc\").mode(\"append\").option(\"url\", jdbc_url).option(\"dbtable\", dbtable).save()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Output Dataframe to Azure SQL DB\ndf_sqldb = df_transform.writeStream\\\n  .trigger(once=True)\\\n  .foreachBatch(output_sqldb)\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", \"abfss://checkpointcontainer@dstore.dfs.core.windows.net/checkpointdb\")\\\n  .start()"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"autoloader_etl_notebook_blog","notebookId":1042538127863659},"nbformat":4,"nbformat_minor":0}