{"cells":[{"cell_type":"markdown","source":["1. Create an Event Hub instance in the previously created Azure Event Hub namespace.\n2. Create a new Shared Access Policy in the Event Hub instance. Copy the connection string generated with the new policy. Note that this connection string has an \"EntityPath\" component , unlike the RootManageSharedAccessKey connectionstring for the Event Hub namespace.\n3. To enable Azure Event hub Databricks ingestion and transformations, install the Azure Event Hubs Connector for Apache Spark from the Maven repository. For this post, I have installed the version 2.3.18 of the connector, using the following  maven coordinate: \"com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18\". This library is the most current package at the time of this writing."],"metadata":{}},{"cell_type":"code","source":["# Permission is based on File or folder based ACL assignments to the Data Lake filesystem (container) . RBAC assignments to the top level Azure Data Lake resource is not required.\nspark.conf.set(\"fs.azure.account.auth.type.adlstore.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.adlstore.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.adlstore.dfs.core.windows.net\", dbutils.secrets.get(\"myscope\", key=\"clientid\"))\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.adlstore.dfs.core.windows.net\", dbutils.secrets.get(\"myscope\", key=\"clientsecret\"))\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adlstore.dfs.core.windows.net\", \"https://login.microsoftonline.com/{}/oauth2/token\".format(dbutils.secrets.get(\"myscope\", key=\"tenantid\")))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Read stream from Azure Event Hub as streaming dataframe using readStream().\nSet your namespace, entity, policy name, and key for Azure Event Hub in the following command."],"metadata":{}},{"cell_type":"code","source":["connectionString = dbutils.secrets.get(\"myscope\", key=\"eventhubconnstr\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Initialize event hub config dictionary with connectionString\nehConf = {}\nehConf['eventhubs.connectionString'] = connectionString"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Add consumer group to the ehConf dictionary\nehConf['eventhubs.consumerGroup'] = \"$Default\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Encrypt ehConf connectionString property\nehConf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["df = spark.readStream.format(\"eventhubs\").options(**ehConf).load()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Write streams into defined sink\nfrom pyspark.sql.types import *\nimport  pyspark.sql.functions as F\n\nevents_schema = StructType([\n  StructField(\"id\", StringType(), True),\n  StructField(\"timestamp\", StringType(), True),\n  StructField(\"uv\", StringType(), True),\n  StructField(\"temperature\", StringType(), True),\n  StructField(\"humidity\", StringType(), True)])\n\ndecoded_df = df.select(F.from_json(F.col(\"body\").cast(\"string\"), events_schema).alias(\"Payload\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(decoded_df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df_events = decoded_df.select(decoded_df.Payload.id, decoded_df.Payload.timestamp, decoded_df.Payload.uv, decoded_df.Payload.temperature, decoded_df.Payload.humidity)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(df_events)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df_out = df_events.writeStream\\\n  .format(\"json\")\\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", \"abfss://checkpointcontainer@adlstore.dfs.core.windows.net/checkpointapievents\")\\\n  .start(\"abfss://api-eventhub@adlstore.dfs.core.windows.net/writedata\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# read back the data for visual exploration\ndf_read = spark.read.format(\"json\").load(\"abfss://api-eventhub@adlstore.dfs.core.windows.net/writedata\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(df_read)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["df_read.printSchema()"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"eventhub_pyspark_streaming_blog","notebookId":1950836122874742},"nbformat":4,"nbformat_minor":0}